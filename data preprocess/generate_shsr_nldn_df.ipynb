{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import zarr\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "storage_account_name = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')\n",
    "sas_token = os.getenv('AZURE_STORAGE_SAS_TOKEN')\n",
    "container_name = os.getenv('CONTAINER_NAME')\n",
    "\n",
    "azure_url=os.getenv('AZURE_URL')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "file_path_pre=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Helper Functions\n",
    "1. to read data from azure blob:\n",
    "    read_zarr_from_blob(account_name, container_name, blob_name, sas_token)\n",
    "2. to generate a list of date in yyyymmdd format for reading a single day's blob file:\n",
    "    generateTimeStamp(start_month, end_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr_from_blob(account_name, container_name, blob_name, sas_token)-> xr:\n",
    "    \"\"\"\n",
    "        account_name : Azure Account Name\n",
    "        container_name: Azure Container Name\n",
    "        blob_name: Azure Blob Name\n",
    "        sas_token: SAS token for certain blob \n",
    "        return: an xarray dataset\n",
    "    \"\"\"\n",
    "    blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=sas_token)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    zarr_store = zarr.ABSStore(prefix=blob_name, client=container_client)\n",
    "    ds = xr.open_zarr(zarr_store)\n",
    "    return ds\n",
    "\n",
    "def generateTimeStamp(year:int, start_month, end_month) -> list:\n",
    "    \"\"\"\n",
    "        year: int(yyyy)\n",
    "        start_month: int(m)\n",
    "        end_month: int(m)\n",
    "        return : list of datetime in format: str(\"yyyymmdd\")\n",
    "    \"\"\"\n",
    "    datetime_list = []\n",
    "    if int(year)%4!=0:\n",
    "        invalid_feb_days = [\"29\",\"30\",\"31\"]\n",
    "    else:\n",
    "        invalid_feb_days = [\"30\",\"31\"]\n",
    "    \n",
    "    for i in range(start_month, end_month + 1):\n",
    "        month = \"0\" + str(i)\n",
    "        for j in range(1, 31):\n",
    "            if 1 <= j < 10:\n",
    "                day = \"0\" + str(j)\n",
    "            else:\n",
    "                day = str(j)\n",
    "            if month in ('04','06','09','11') and day == \"31\" or  (month=='02' and day in invalid_feb_days):\n",
    "                continue\n",
    "\n",
    "            datetime = str(year) + month + day\n",
    "            datetime_list.append(datetime)\n",
    "    return datetime_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read the NLDN data\n",
    "\n",
    "1. read the original NLDN data: \n",
    "    get_NLDN_lightning_Data(start_date, end_date)\n",
    "2. create lightning_mean, lightning_sum in 0.25 spatial degree: \n",
    "    get_lightning_sum_per_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NLDN lightning data for CONUS\n",
    "\"\"\"\n",
    "    start_date: \"yyyy-mm-dd\"\n",
    "    end_date: \"yyyy-mm-dd\"\n",
    "    return : xarray dataset of NLDN\n",
    "\"\"\"\n",
    "def get_NLDN_lightning_Data(start_date:str, end_date:str) ->xr:\n",
    "    blob_file_name = \"data/lightning.zarr\"\n",
    "    nldn_ds = read_zarr_from_blob(storage_account_name, container_name,blob_file_name, sas_token)\n",
    "    ds = nldn_ds.sel(\n",
    "            latitude=slice(50, 25),\n",
    "            longitude=slice(235, 295),\n",
    "            time=slice(start_date, end_date)) \n",
    "    return ds\n",
    "\n",
    "# get the sum of lightning count for each day across the CONUS for the date range\n",
    "def get_lightning_sum_per_day(start_date:str, end_date:str) ->xr:\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "        return nldn dataset with lightning sum and lightning mean\n",
    "    \"\"\"\n",
    "    total_nldn_ds = get_NLDN_lightning_Data(start_date,end_date)\n",
    "    sum_lightning = total_nldn_ds.lightning.sum(dim=['latitude','longitude']).compute()\n",
    "    mean_lightning = total_nldn_ds.lightning.mean(dim=['latitude','longitude']).compute()\n",
    "    total_nldn_ds['lightning_sum'] = sum_lightning\n",
    "    total_nldn_ds['lightning_mean'] = mean_lightning\n",
    "    return total_nldn_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Read the SHSR data\n",
    "1. read the original SHSR data, in 2 min time window, and 0.02 spatial degree:\n",
    "    get_total_SHSR_ds(year:int, start_month:int, end_month:int)\n",
    "2. resample the data into 1-Hr time window\n",
    "    simplify_time_range_for_SHSR(ds:xr)\n",
    "3. resample the data into 0.25 spatial degree by coarsen, modify the latitude/longitude round it to 0.25 degree with \"linear\":\n",
    "    get_max_date_SHSR_in_hr_range_and_quarter_degree_range_result(ds:xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the SHSR radar dateset from date range(2022.07.01 - 2022.08.31)\n",
    "def get_total_SHSR_ds(year:int, start_month:int, end_month:int)->xr:\n",
    "    \"\"\"\n",
    "        year: int(yyyy)\n",
    "        start_month: int(m)\n",
    "        end_month: int(m)\n",
    "        return : an xarray dataset of SHSR data from start_month to end_month in the same year\n",
    "    \"\"\"\n",
    "    datetime_list = generateTimeStamp(year, start_month, end_month)\n",
    "    concatenated_list = []\n",
    "    for datetime in datetime_list:\n",
    "        blob_file_name = \"data/SHSR/\" + datetime + \".zarr\"\n",
    "        current_ds = read_zarr_from_blob(\n",
    "            storage_account_name, container_name, blob_file_name,sas_token\n",
    "        )\n",
    "        concatenated_list.append(current_ds)\n",
    "\n",
    "    concatenated_ds = xr.concat(concatenated_list, dim=\"time\")\n",
    "    return concatenated_ds\n",
    "\n",
    "def simplify_time_range_for_SHSR(ds:xr) ->xr:\n",
    "    \"\"\"\n",
    "        input ds is the original merged dataset including all the SHSR info in a certain time window\n",
    "        the time slice for each record is 2mins\n",
    "        this function resample the time slice into 1 Hr window, \n",
    "        only filter the positive SHSR value, to ensure the mean/std/sum/ct are still valid after aggregation\n",
    "    \"\"\"\n",
    "    ds_lightning_positive = ds.where(ds[\"SHSR\"] > 0)\n",
    "    ds_resampled_mean = ds_lightning_positive.resample(time=\"1H\").mean().rename({\"SHSR\": \"SHSR_mean\"})\n",
    "    ds_resampled_std = ds_lightning_positive.resample(time=\"1H\").std().rename({\"SHSR\": \"SHSR_std\"}) \n",
    "    # ds_resampled_sum = ds_lightning_positive.resample(time=\"1H\").sum().rename({\"SHSR\": \"SHSR_sum\"})\n",
    "    ds_resampled_ct = ds_lightning_positive.resample(time=\"1H\").count().rename({\"SHSR\": \"SHSR_ct\"})\n",
    "    ds_resampled_diff = (ds_lightning_positive.resample(time=\"1H\").max()- ds_lightning_positive.resample(time=\"1H\").min()).rename({\"SHSR\": \"SHSR_diff\"})\n",
    "\n",
    "    ds_shsr_above_30_ct = ds_lightning_positive.where(ds_lightning_positive[\"SHSR\"] > 30)\n",
    "    ds_resampled_above_30_ct = ds_shsr_above_30_ct.resample(time=\"1H\").count().rename({\"SHSR\": \"SHSR_above_30_ct\"})\n",
    "    \n",
    "    ds_shsr_above_40_ct = ds_lightning_positive.where(ds_lightning_positive[\"SHSR\"] > 40)\n",
    "    ds_resampled_above_40_ct = ds_shsr_above_40_ct.resample(time=\"1H\").count().rename({\"SHSR\": \"SHSR_above_40_ct\"})\n",
    "    \n",
    "    ds_shsr_above_50_ct = ds_lightning_positive.where(ds_lightning_positive[\"SHSR\"] > 50)\n",
    "    ds_resampled_above_50_ct = ds_shsr_above_50_ct.resample(time=\"1H\").count().rename({\"SHSR\": \"SHSR_above_50_ct\"})\n",
    "    \n",
    "    \n",
    "\n",
    "    ds_merged = xr.merge([\n",
    "            ds_resampled_mean,\n",
    "            ds_resampled_std,\n",
    "            ds_resampled_ct,\n",
    "            ds_resampled_diff,\n",
    "            ds_resampled_above_30_ct,ds_resampled_above_40_ct,ds_resampled_above_50_ct])\n",
    "\n",
    "    return ds_merged\n",
    "\n",
    "\n",
    "def get_total_SHSR_ds_in_hr_range(year:int, start_month:int, end_month:int)->xr:\n",
    "    \"\"\" \n",
    "        this function set up to stagely store the SHSR dataset in Hourly time window\n",
    "    \"\"\"\n",
    "    original_ds = get_total_SHSR_ds(year,start_month,end_month)\n",
    "    # original_ds = get_total_SHSR_ds(2022,7,8)\n",
    "    ds = simplify_time_range_for_SHSR(original_ds)\n",
    "    return ds\n",
    "\n",
    "def get_max_date_SHSR_in_hr_range_and_quarter_degree_range(year:int, start_month:int, end_month:int, expend_ratio:int):\n",
    "    \"\"\" \n",
    "        this function set up to aggreate the geological information from 0.2 degree to 0.25 degree\n",
    "        mainly using the coarsen function, with a \"trim\" boundary to reduce the overlap region of each new grid(0.25*0.25)\n",
    "        \n",
    "    \"\"\"\n",
    "    ds_interp = get_total_SHSR_ds_in_hr_range(year,start_month,end_month)\n",
    "    # in our case the expend ration should be 13\n",
    "    coarsen_factor = {'latitude': expend_ratio, 'longitude': expend_ratio}\n",
    "    # Avg\n",
    "    ds_coarsened_avg = ds_interp.coarsen(dim=coarsen_factor, boundary='trim').mean()\n",
    "    new_var_names_avg = {var_name: f\"avg_{var_name}\" for var_name in ds_coarsened_avg.data_vars}\n",
    "    ds_coarsened_avg = ds_coarsened_avg.rename(new_var_names_avg)\n",
    "    \n",
    "    # Sum\n",
    "    ds_coarsened_sum = ds_interp.coarsen(dim=coarsen_factor, boundary='trim').sum()\n",
    "    new_var_names_sum = {var_name: f\"sum_{var_name}\" for var_name in ds_coarsened_sum.data_vars}\n",
    "    ds_coarsened_sum = ds_coarsened_sum.rename(new_var_names_sum)\n",
    "\n",
    "    # Std\n",
    "    ds_coarsened_std = ds_interp.coarsen(dim=coarsen_factor, boundary='trim').std()\n",
    "    new_var_names_std = {var_name: f\"std_{var_name}\" for var_name in ds_coarsened_std.data_vars}\n",
    "    ds_coarsened_std = ds_coarsened_std.rename(new_var_names_std)\n",
    "\n",
    "    # Min\n",
    "    ds_coarsened_min = ds_interp.coarsen(dim=coarsen_factor, boundary='trim').min()\n",
    "    new_var_names_min = {var_name: f\"min_{var_name}\" for var_name in ds_coarsened_min.data_vars}\n",
    "    ds_coarsened_min = ds_coarsened_min.rename(new_var_names_min)\n",
    "\n",
    "    # Max\n",
    "    ds_coarsened_max = ds_interp.coarsen(dim=coarsen_factor, boundary='trim').max()\n",
    "    new_var_names_max = {var_name: f\"max_{var_name}\" for var_name in ds_coarsened_max.data_vars}\n",
    "    ds_coarsened_max = ds_coarsened_max.rename(new_var_names_max)\n",
    "    \n",
    "    ds_coarsened = xr.merge([ds_coarsened_avg, ds_coarsened_sum, ds_coarsened_std, ds_coarsened_min, ds_coarsened_max])\n",
    "\n",
    "    return ds_coarsened\n",
    "\n",
    "def get_max_date_SHSR_in_hr_range_and_quarter_degree_range_result(ds_interp):\n",
    "    \n",
    "    new_longitude = np.around(ds_interp[\"longitude\"] / 0.25) * 0.25\n",
    "    new_latitude = np.around(ds_interp[\"latitude\"] / 0.25) * 0.25\n",
    "\n",
    "    ds_interp = ds_interp.interp(\n",
    "        latitude=new_latitude, longitude=new_longitude, method=\"linear\"\n",
    "    )\n",
    "\n",
    "    ds_interp[\"longitude\"] = np.around(ds_interp[\"longitude\"] / 0.25) * 0.25\n",
    "    ds_interp[\"latitude\"] = np.around(ds_interp[\"latitude\"] / 0.25) * 0.25\n",
    "    \n",
    "    return ds_interp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Merge the NLDN and SHSR data, prepare for training set with 3 different rules:\n",
    "1. data with positive lightning event and positive SHSR value in 0.25 degree & 1hr time window:\n",
    "    getLightning_positive_lightning_shsr_positive(df)\n",
    "2. data with positive SHSR value but non lightning event in 0.25 degree & 1hr time window:\n",
    "    getLightning_positive_shsr_0_lightning(df)\n",
    "3. data with positive lightning event and Non SHSR value in 0.25 degree & 1hr time window:\n",
    "    getLightning_positive_df_0_shsr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nldn_and_SHSR_ds(year:int, start_month:int, end_month:int, expend_ratio:int):\n",
    "    \"\"\"\n",
    "        Only consider the NLDN and SHSR data has intersection on the same location.\n",
    "    \n",
    "    \"\"\"\n",
    "    ds_shsr= get_max_date_SHSR_in_hr_range_and_quarter_degree_range(year, start_month, end_month, expend_ratio)\n",
    "    ds_shsr_hr_large = get_max_date_SHSR_in_hr_range_and_quarter_degree_range_result(ds_shsr)\n",
    "    start_date = str(year)+\"-\"+str(start_month)+\"-01\"\n",
    "    if end_month in (4,6,9,11):\n",
    "        endDay = 30\n",
    "    elif end_month == 2 and year%4==0:\n",
    "        endDay = 29\n",
    "    elif end_month ==2 and year%4!=0:\n",
    "        endDay = 28\n",
    "    else:\n",
    "        endDay = 31\n",
    "    end_date = str(year)+\"-\"+str(start_month)+\"-\"+str(endDay)\n",
    "    nldn_ds = get_lightning_sum_per_day(start_date, end_date)\n",
    "    shsr_ds_unique = ds_shsr_hr_large.drop_duplicates(dim=[\"latitude\", \"longitude\",'time'])\n",
    "    common_latitudes = xr.DataArray(list(set(nldn_ds.latitude.values) & set(shsr_ds_unique.latitude.values)), dims=[\"latitude\"])\n",
    "    common_longitudes = xr.DataArray(list(set(nldn_ds.longitude.values) & set(shsr_ds_unique.longitude.values)), dims=[\"longitude\"])\n",
    "    shsr_filtered = shsr_ds_unique.sel(latitude=common_latitudes, longitude=common_longitudes, method='nearest')\n",
    "    merged_ds = xr.merge([shsr_filtered, nldn_ds])\n",
    "    return merged_ds\n",
    "    \n",
    "    \n",
    "def getLightning_positive_lightning_shsr_positive(df):\n",
    "    df_lightning_positive = df[df['lightning'] > 0]\n",
    "    df_return = df_lightning_positive[df_lightning_positive[\"avg_SHSR_mean\"]>0]\n",
    "    return df_return\n",
    "\n",
    "# def getLightning_positive_df_0_shsr(df):\n",
    "#     df_lightning_positive = df[df['lightning'] > 0]\n",
    "#     df_lightning_positive['avg_SHSR_mean'] = df_lightning_positive['avg_SHSR_mean'].fillna(value=0) \n",
    "#     df_return = df_lightning_positive.dropna(subset=['avg_SHSR_mean'])\n",
    "#     return df_return    \n",
    "\n",
    "def getLightning_positive_shsr_0_lightning(df):\n",
    "    df_shsr_positive = df[df['avg_SHSR_mean'] > 0]\n",
    "    df_shsr_positive['lightning'] = df_shsr_positive['lightning'].fillna(value=0) \n",
    "    df_return = df_shsr_positive.dropna(subset=['lightning'])\n",
    "    return df_return    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. merge the NLDN data and SHSR data together, and generate a dataframe, meanwhile save each day's infomation into csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_between(start, end):\n",
    "    \"\"\"\n",
    "        start: \"yyyy-mm-dd\"\n",
    "        end: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "    date_list = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') for x in range((end_date - start_date).days + 1)]\n",
    "    return date_list\n",
    "\n",
    "def save_date_csv(single_date, merged_ds):\n",
    "    \"\"\"\n",
    "        single_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    \n",
    "    ds_daily = merged_ds.sel(time=slice(single_date, single_date))\n",
    "    ds_daily.attrs['date'] = single_date\n",
    "    df = ds_daily.load().to_dataframe()\n",
    "    df_lightning1_shsr_1 = getLightning_positive_lightning_shsr_positive(df)\n",
    "    csv_file_path = 'total_merged_df_positive_' + single_date+ '.csv'\n",
    "    df_lightning1_shsr_1.to_csv(csv_file_path)\n",
    "    print(single_date, \"is completed\")\n",
    "    \n",
    "\n",
    "def save_total_csv(start_date:str, end_date:str, merged_ds):\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    date_list = generate_date_between(start_date, end_date)\n",
    "    for date in date_list:\n",
    "        save_date_csv(date, merged_ds)\n",
    "    \n",
    "\n",
    "\n",
    "def read_csv_to_df(start_date:str, end_date:str, file_path_pre):\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    datelist = generate_date_between(start_date, end_date)\n",
    "    file_path_prefix = file_path_pre\n",
    "    df_list = []\n",
    "    for date in datelist:\n",
    "        # if date == \"2021-07-31\":\n",
    "        #     continue\n",
    "        filename = \"total_merged_df_positive_\"+date+\".csv\"\n",
    "        path = file_path_prefix+filename\n",
    "        print(path)\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    merged_df = pd.concat(df_list, axis=0)\n",
    "    merged_df['time'] = pd.to_datetime(merged_df['time'])\n",
    "    return merged_df\n",
    "\n",
    "def save_date_csv_negative(single_date, merged_ds):\n",
    "    \"\"\"\n",
    "        single_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    \n",
    "    ds_daily = merged_ds.sel(time=slice(single_date, single_date))\n",
    "    ds_daily.attrs['date'] = single_date\n",
    "    df = ds_daily.load().to_dataframe()\n",
    "    df_lightning1_shsr_1 = getLightning_positive_shsr_0_lightning(df)\n",
    "    \n",
    "    csv_file_path = 'negative_total_merged_df_' + single_date+ '.csv'\n",
    "    df_lightning1_shsr_1.to_csv(csv_file_path)\n",
    "    print(single_date, \"is completed\")\n",
    "\n",
    "def save_total_csv_negative(start_date:str, end_date:str, merged_ds):\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    date_list = generate_date_between(start_date, end_date)\n",
    "    for date in date_list:\n",
    "        save_date_csv_negative(date, merged_ds)\n",
    "\n",
    "def read_csv_to_df_negative(start_date:str, end_date:str, file_path_pre):\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    datelist = generate_date_between(start_date, end_date)\n",
    "    file_path_prefix = file_path_pre\n",
    "    df_list = []\n",
    "    for date in datelist:\n",
    "        filename = \"negative_total_merged_df_\"+date+\".csv\"\n",
    "        path = file_path_prefix+filename\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    merged_df = pd.concat(df_list, axis=0)\n",
    "    merged_df['time'] = pd.to_datetime(merged_df['time'])\n",
    "    return merged_df \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data for 2021-07 to 2021-08\n",
    "merged_ds_2021_7_8 = merge_nldn_and_SHSR_ds(2021, 7, 8, 13)\n",
    "save_total_csv(\"2021-07-01\",'2021-08-30',merged_ds_2021_7_8)\n",
    "save_total_csv_negative(\"2021-07-01\",'2021-08-30',merged_ds_2021_7_8)\n",
    "\n",
    "#Ge data fro 2022-07 to 2022-08\n",
    "merged_ds_2022_7_8 = merge_nldn_and_SHSR_ds(2022, 7, 8, 13)\n",
    "save_total_csv(\"2022-07-01\",'2022-08-30',merged_ds_2022_7_8)\n",
    "save_total_csv_negative(\"2022-07-01\",'2022-08-30',merged_ds_2022_7_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Read the weather data and merged with NLDN+SHSR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get single date of weather radar dataset\n",
    "def get_weather_at_year(weatherType:str, year:int)->xr:\n",
    "    \n",
    "    \"\"\"\n",
    "        weatherType: str(\"cloud\", \"wind_v\",\"wind_u\",\"temperature\",\"dewpoint\")\n",
    "        year: int(yyyy)\n",
    "    \"\"\"\n",
    "    if weatherType.lower() == \"cloud\":\n",
    "        fileType = \"total_cloud_cover_\"+str(year)\n",
    "    elif weatherType.lower() == \"temperature\":\n",
    "        fileType = \"2m_temperature_\"+str(year)\n",
    "    elif weatherType.lower() == \"dewpoint\":\n",
    "        fileType = \"2m_dewpoint_temperature_\"+str(year)\n",
    "    elif weatherType.lower() == \"sea_pressure\":\n",
    "        fileType = \"mean_sea_level_pressure_\"+str(year)\n",
    "    elif weatherType.lower() == \"wind_u\":\n",
    "        fileType = \"10m_u_component_of_wind_\"+str(year)\n",
    "    elif weatherType.lower() == \"wind_v\":   \n",
    "        fileType = \"10m_v_component_of_wind_\"+str(year)\n",
    "        \n",
    "    fileName =  fileType+\".zarr\"\n",
    "    blob_file_name1 = \"data/ERA5/\"+fileName\n",
    "    current_ds = read_zarr_from_blob(\n",
    "        storage_account_name, container_name, blob_file_name1,sas_token\n",
    "    )\n",
    "    return current_ds\n",
    "\n",
    "def get_weather_ds_in_daterange(start_date:str, end_date:str, year:int)->xr:\n",
    "    \"\"\"\n",
    "        start_date: \"yyyy-mm-dd\"\n",
    "        end_date: \"yyyy-mm-dd\"\n",
    "    \"\"\"\n",
    "    cloud_2022_ds = get_weather_at_year(\"cloud\", year)\n",
    "    wind_2022_v_ds = get_weather_at_year(\"wind_v\",year)\n",
    "    wind_2022_u_ds = get_weather_at_year(\"wind_u\",year)\n",
    "    temperature_2022 = get_weather_at_year(\"temperature\",year)\n",
    "    dewpoint_2022 = get_weather_at_year(\"dewpoint\",year)\n",
    "    \n",
    "    cloud_month_ds = cloud_2022_ds.sel(time = slice(start_date, end_date)) #tcc\n",
    "    wind_month_v = wind_2022_v_ds.sel(time = slice(start_date, end_date)) #v10\n",
    "    wind_month_u = wind_2022_u_ds.sel(time = slice(start_date, end_date)) #u10\n",
    "    temperature_month = temperature_2022.sel(time = slice(start_date, end_date)) \n",
    "    dewpoint_month = dewpoint_2022.sel(time = slice(start_date, end_date)) #d2m\n",
    "    merged_ds = xr.merge([cloud_month_ds, wind_month_v, wind_month_u, temperature_month, dewpoint_month])\n",
    "    \n",
    "    \n",
    "    return merged_ds\n",
    "\n",
    "def get_weather_df(ds:xr) ->pd:\n",
    "    \n",
    "    weather_df = ds.to_dataframe()\n",
    "    weather_df = weather_df[['tcc', 'valid_time', 'v10', 'u10', 't2m','d2m']]\n",
    "    weather_df.reset_index(inplace=True)\n",
    "    weather_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return weather_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. adding more features, previous 4 hours' info, and future 4 hours' info, also static info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geographic_local_time(longitude, utc_datetime):\n",
    "    timezone_offset = timedelta(hours=longitude / 15)\n",
    "    local_time = utc_datetime + timezone_offset\n",
    "    return local_time\n",
    "\n",
    "def floor_to_nearest_hour(dt):\n",
    "    return dt.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "def get_season(latitude, local_time):\n",
    "    month = local_time.month\n",
    "    # Northern Hemisphere \n",
    "    if latitude >= 0:  \n",
    "        if 3 <= month <= 5:\n",
    "            return 'Spring'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'Summer'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'Autumn'\n",
    "        else:\n",
    "            return 'Winter'\n",
    "    else:  # Southern Hemisphere\n",
    "        if 3 <= month <= 5:\n",
    "            return 'Autumn'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'Winter'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'Spring'\n",
    "        else:\n",
    "            return 'Summer'\n",
    "        \n",
    "def add_static_var(df):\n",
    "\n",
    "    local_times = []\n",
    "    local_seasons = []\n",
    "    for index, _ in df.iterrows():\n",
    "        time, latitude, longitude = index\n",
    "        if longitude > 180: longitude -= 360\n",
    "        if latitude >90: latitude-=180\n",
    "        local_time = calculate_geographic_local_time(longitude, time)\n",
    "        local_season = get_season(latitude,time)\n",
    "        local_times.append(local_time)\n",
    "        local_seasons.append(local_season)\n",
    "\n",
    "    # 1. get local time\n",
    "    df['local_time'] = local_times \n",
    "    df['local_time'] = df['local_time'].apply(floor_to_nearest_hour)\n",
    "    # 2. add day of year\n",
    "    df['day_of_year'] = df['local_time'].dt.dayofyear\n",
    "    # 3. add time of day\n",
    "    df['hour_of_day'] = df['local_time'].apply(lambda x: x.hour)\n",
    "    # 4. add local season\n",
    "    df['season'] = local_seasons\n",
    "    df['isSummer'] = df['season'] == 'Summer'\n",
    "    df['isSpring'] = df['season'] == 'Spring'\n",
    "    df['isWinter'] = df['season'] == 'Winter'\n",
    "    df['isAutumn'] = df['season'] == 'Autumn'\n",
    "    df.sort_values(by='local_time', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## deal with weather data\n",
    "def add_weather_feature(df):\n",
    "  df['ws'] = np.sqrt(df['u10']**2 + df['v10']**2)\n",
    "  for var in ['tcc','d2m','t2m','ws']:\n",
    "    # prev 4 hr\n",
    "      df[f'{var}_prev_4hr_sum'] = df[var].shift(1).rolling(window=4, min_periods=1).sum()\n",
    "      df[f'{var}_prev_4hr_std'] = df[var].shift(1).rolling(window=4, min_periods=1).std()\n",
    "    # after 2hr \n",
    "      df[f'{var}_after_2hr_sum'] = df[var].shift(-1)+df[var].shift(-2)\n",
    "    # after 4hr \n",
    "      df[f'{var}_after_4hr_sum'] = df[var].shift(-1)+df[var].shift(-2)+df[var].shift(-3)+df[var].shift(-4)\n",
    "  return df\n",
    "\n",
    "\n",
    "def add_SHSR_feature(df, variables, total_variables):\n",
    "    # For all the variables related to SHSR\n",
    "    vars = variables + total_variables \n",
    "    for var in vars:\n",
    "        # prev 4 hr\n",
    "        df[f'{var}_prev_4hr_sum'] = df[var].shift(1).rolling(window=4, min_periods=1).sum()\n",
    "        # after 2hr \n",
    "        df[f'{var}_after_2hr_sum'] = df[var].shift(-1)+df[var].shift(-2)\n",
    "        # after 4hr \n",
    "        df[f'{var}_after_4hr_sum'] = df[var].shift(-1)+df[var].shift(-2)+df[var].shift(-3)+df[var].shift(-4)\n",
    "\n",
    "    # weather_shsr_var = ['tcc','d2m','t2m','ws'] + total_variables\n",
    "    for var in total_variables:\n",
    "        # 创建四小时前到一小时前的列\n",
    "        for i in range(1, 5):\n",
    "            df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
    "\n",
    "        # 创建一小时后到三小时后的列\n",
    "        if var == \"lightning\":\n",
    "            continue\n",
    "        for i in range(1, 4):\n",
    "            df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
    "\n",
    "    df1 = df.fillna(0)\n",
    "    return df1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_dataframe(year:int,start_date:str,end_date:str,isPositive:bool,file_path_pre:str):\n",
    "    if not isPositive:\n",
    "        shsr_nldn_df = read_csv_to_df_negative(start_date, end_date,file_path_pre)\n",
    "    else:\n",
    "        shsr_nldn_df = read_csv_to_df(start_date, end_date,file_path_pre)\n",
    "    weather_ds  = get_weather_ds_in_daterange(start_date, end_date, year)\n",
    "    weather_df = get_weather_df(weather_ds)\n",
    "    shsr_nldn_df.set_index(['time','latitude','longitude'],inplace=True)\n",
    "    weather_df.set_index(['time','latitude','longitude'],inplace=True)\n",
    "    model_df = pd.merge(shsr_nldn_df, weather_df, left_index=True, right_index=True, how='inner')\n",
    "    return model_df\n",
    "\n",
    "        \n",
    "def save_modeling_dataframe_to_csv(pre_df, isPositve, year):\n",
    "    variables = [col for col in pre_df.columns if 'SHSR' in col]\n",
    "    total_variables = [\n",
    "        'avg_SHSR_mean', 'avg_SHSR_std', \n",
    "        'avg_SHSR_ct', 'avg_SHSR_diff', 'avg_SHSR_above_30_ct', \n",
    "        'avg_SHSR_above_40_ct','avg_SHSR_above_50_ct',\n",
    "        \"max_SHSR_mean\",\"max_SHSR_ct\",\"max_SHSR_diff\", \"min_SHSR_mean\",'lightning'\n",
    "    ]\n",
    "\n",
    "\n",
    "    model = add_static_var(pre_df)\n",
    "    model = add_weather_feature(model)\n",
    "    model = add_SHSR_feature(model,variables, total_variables)\n",
    "    model.dropna(inplace=model)\n",
    "    if isPositve:\n",
    "        fileName = str(year)+\"_positive_total.csv\"\n",
    "    else:\n",
    "        fileName = str(year)+\"_negative_total.csv\"\n",
    "    model.to_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe before adding features in different time-windows\n",
    "model_df_2021_positive = get_final_dataframe(2021,\"2021-07-01\", \"2021-08-30\",True,file_path_pre)\n",
    "model_df_2022_positve = get_final_dataframe(2022,\"2022-07-01\", \"2022-08-30\",True,file_path_pre)\n",
    "model_df_2021_negative = get_final_dataframe(2021,\"2021-07-01\", \"2021-08-30\",False,file_path_pre)\n",
    "model_df_2022_negative  = get_final_dataframe(2022,\"2022-07-01\", \"2022-08-30\",False,file_path_pre)\n",
    "\n",
    "# save the final modeling dataframe with 305 columns\n",
    "save_modeling_dataframe_to_csv(model_df_2021_positive, True, 2021)\n",
    "save_modeling_dataframe_to_csv(model_df_2022_positve, True, 2022)\n",
    "save_modeling_dataframe_to_csv(model_df_2021_negative, False, 2021)\n",
    "save_modeling_dataframe_to_csv(model_df_2022_negative, False, 2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
